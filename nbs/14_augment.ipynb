{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp augment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T04:31:23.942660444Z",
     "start_time": "2023-06-10T04:31:23.110144424Z"
    }
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch,random\n",
    "import fastcore.all as fc\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "from tonyai.datasets import *\n",
    "from tonyai.conv import *\n",
    "from tonyai.learner import *\n",
    "from tonyai.activations import *\n",
    "from tonyai.init import *\n",
    "from tonyai.sgd import *\n",
    "from tonyai.resnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T04:31:23.945974750Z",
     "start_time": "2023-06-10T04:31:23.944163374Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil\n",
    "import matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import torchvision.transforms.functional as TF,torch.nn.functional as F\n",
    "from torch import tensor,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "from fastcore.test import test_close\n",
    "from torch import distributions\n",
    "\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray_r'\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "if fc.defaults.cpus>8: fc.defaults.cpus=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T04:31:33.212980523Z",
     "start_time": "2023-06-10T04:31:32.299942617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "25aaf24e26844e78921951892b6ea735"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xl,yl = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "bs = 1024\n",
    "xmean,xstd = 0.28, 0.35\n",
    "\n",
    "@inplace\n",
    "def transformi(b): b[xl] = [(TF.to_tensor(o)-xmean)/xstd for o in b[xl]]\n",
    "\n",
    "dsd = load_dataset(name)\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T04:31:36.710028362Z",
     "start_time": "2023-06-10T04:31:36.703110235Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "astats = ActivationStats(fc.risinstance(GeneralRelu))\n",
    "cbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\n",
    "act_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\n",
    "iw = partial(init_weights, leaky=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T04:31:40.202038419Z",
     "start_time": "2023-06-10T04:31:40.189486423Z"
    }
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "lr,epochs = 6e-2,5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going wider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T04:31:41.777632998Z",
     "start_time": "2023-06-10T04:31:41.771222918Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(act=nn.ReLU, nfs=(16,32,64,128,256,512), norm=nn.BatchNorm2d):\n",
    "    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [nn.Flatten(), nn.Linear(nfs[-1], 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T04:31:43.042595484Z",
     "start_time": "2023-06-10T04:31:43.011059706Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "model = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T04:27:44.121200485Z",
     "start_time": "2023-06-10T04:27:34.510290358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ˜€Using device cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73817b0996594898bca783b4264fa675"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m matplotlib\u001B[38;5;241m.\u001B[39muse(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQt5Agg\u001B[39m\u001B[38;5;124m'\u001B[39m) \n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[43mlearn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/tonyai/tonyai/learner.py:181\u001B[0m, in \u001B[0;36mLearner.fit\u001B[0;34m(self, n_epochs, train, valid, cbs, lr)\u001B[0m\n\u001B[1;32m    179\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m lr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m: lr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mopt_func: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mopt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mopt_func(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mparameters(), lr)\n\u001B[0;32m--> 181\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m cb \u001B[38;5;129;01min\u001B[39;00m cbs: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcbs\u001B[38;5;241m.\u001B[39mremove(cb)\n",
      "File \u001B[0;32m~/tonyai/tonyai/learner.py:132\u001B[0m, in \u001B[0;36mwith_cbs.__call__.<locals>._f\u001B[0;34m(o, *args, **kwargs)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    131\u001B[0m     o\u001B[38;5;241m.\u001B[39mcallback(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbefore_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 132\u001B[0m     \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    133\u001B[0m     o\u001B[38;5;241m.\u001B[39mcallback(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mafter_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;28mglobals\u001B[39m()[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCancel\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnm\u001B[38;5;241m.\u001B[39mtitle()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mException\u001B[39m\u001B[38;5;124m'\u001B[39m]: \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m~/tonyai/tonyai/learner.py:169\u001B[0m, in \u001B[0;36mLearner._fit\u001B[0;34m(self, train, valid)\u001B[0m\n\u001B[1;32m    166\u001B[0m \u001B[38;5;129m@with_cbs\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, train, valid):\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepochs:\n\u001B[0;32m--> 169\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m train: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mone_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m valid: torch\u001B[38;5;241m.\u001B[39mno_grad()(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mone_epoch)(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/tonyai/tonyai/learner.py:164\u001B[0m, in \u001B[0;36mLearner.one_epoch\u001B[0;34m(self, training)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtrain(training)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdl \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdls\u001B[38;5;241m.\u001B[39mtrain \u001B[38;5;28;01mif\u001B[39;00m training \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdls\u001B[38;5;241m.\u001B[39mvalid\n\u001B[0;32m--> 164\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/tonyai/tonyai/learner.py:132\u001B[0m, in \u001B[0;36mwith_cbs.__call__.<locals>._f\u001B[0;34m(o, *args, **kwargs)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    131\u001B[0m     o\u001B[38;5;241m.\u001B[39mcallback(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbefore_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 132\u001B[0m     \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    133\u001B[0m     o\u001B[38;5;241m.\u001B[39mcallback(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mafter_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;28mglobals\u001B[39m()[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCancel\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnm\u001B[38;5;241m.\u001B[39mtitle()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mException\u001B[39m\u001B[38;5;124m'\u001B[39m]: \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m~/tonyai/tonyai/learner.py:159\u001B[0m, in \u001B[0;36mLearner._one_epoch\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;129m@with_cbs\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_one_epoch\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 159\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdl): \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/tonyai/tonyai/learner.py:132\u001B[0m, in \u001B[0;36mwith_cbs.__call__.<locals>._f\u001B[0;34m(o, *args, **kwargs)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    131\u001B[0m     o\u001B[38;5;241m.\u001B[39mcallback(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbefore_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 132\u001B[0m     \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    133\u001B[0m     o\u001B[38;5;241m.\u001B[39mcallback(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mafter_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;28mglobals\u001B[39m()[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCancel\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnm\u001B[38;5;241m.\u001B[39mtitle()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mException\u001B[39m\u001B[38;5;124m'\u001B[39m]: \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m~/tonyai/tonyai/learner.py:146\u001B[0m, in \u001B[0;36mLearner._one_batch\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;129m@with_cbs\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_one_batch\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 146\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mafter_predict\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_loss()\n",
      "File \u001B[0;32m~/tonyai/tonyai/learner.py:196\u001B[0m, in \u001B[0;36mTrainLearner.predict\u001B[0;34m(self)\u001B[0m\n\u001B[0;32m--> 196\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m): \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 204\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/tonyai/tonyai/resnet.py:47\u001B[0m, in \u001B[0;36mResBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[0;32m---> 47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x): \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43midconv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpool\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1215\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks:\n\u001B[1;32m   1214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m*\u001B[39m_global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[0;32m-> 1215\u001B[0m         hook_result \u001B[38;5;241m=\u001B[39m \u001B[43mhook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1216\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m hook_result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1217\u001B[0m             result \u001B[38;5;241m=\u001B[39m hook_result\n",
      "File \u001B[0;32m~/tonyai/tonyai/activations.py:52\u001B[0m, in \u001B[0;36mHooksCallback._hookfunc\u001B[0;34m(self, learn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_hookfunc\u001B[39m(\u001B[38;5;28mself\u001B[39m, learn, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 52\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_train \u001B[38;5;129;01mand\u001B[39;00m learn\u001B[38;5;241m.\u001B[39mtraining) \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_valid \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m learn\u001B[38;5;241m.\u001B[39mtraining): \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhookfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/tonyai/tonyai/activations.py:64\u001B[0m, in \u001B[0;36mappend_stats\u001B[0;34m(hook, mod, inp, outp)\u001B[0m\n\u001B[1;32m     62\u001B[0m hook\u001B[38;5;241m.\u001B[39mstats[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(acts\u001B[38;5;241m.\u001B[39mmean())\n\u001B[1;32m     63\u001B[0m hook\u001B[38;5;241m.\u001B[39mstats[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(acts\u001B[38;5;241m.\u001B[39mstd())\n\u001B[0;32m---> 64\u001B[0m hook\u001B[38;5;241m.\u001B[39mstats[\u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(\u001B[43macts\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mabs\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhistc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m40\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "learn.fit(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-10T04:22:57.666199854Z"
    }
   },
   "outputs": [],
   "source": [
    "class GlobalAvgPool(nn.Module):\n",
    "    def forward(self, x): return x.mean((-2,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-10T04:22:57.798749940Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model2(act=nn.ReLU, nfs=(16,32,64,128,256), norm=nn.BatchNorm2d):\n",
    "    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [ResBlock(256, 512, act=act, norm=norm), GlobalAvgPool()]\n",
    "    layers += [nn.Linear(512, 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _flops(x, h, w):\n",
    "    if x.dim()<3: return x.numel()\n",
    "    if x.dim()==4: return x.numel()*h*w\n",
    "\n",
    "@fc.patch\n",
    "def summary(self:Learner):\n",
    "    res = '|Module|Input|Output|Num params|MFLOPS|\\n|--|--|--|--|--|\\n'\n",
    "    totp,totf = 0,0\n",
    "    def _f(hook, mod, inp, outp):\n",
    "        nonlocal res,totp,totf\n",
    "        nparms = sum(o.numel() for o in mod.parameters())\n",
    "        totp += nparms\n",
    "        *_,h,w = outp.shape\n",
    "        flops = sum(_flops(o, h, w) for o in mod.parameters())/1e6\n",
    "        totf += flops\n",
    "        res += f'|{type(mod).__name__}|{tuple(inp[0].shape)}|{tuple(outp.shape)}|{nparms}|{flops:.1f}|\\n'\n",
    "    with Hooks(self.model, _f) as hooks: self.fit(1, lr=1, cbs=SingleBatchCB())\n",
    "    print(f\"Tot params: {totp}; MFLOPS: {totf:.1f}\")\n",
    "    if fc.IN_NOTEBOOK:\n",
    "        from IPython.display import Markdown\n",
    "        return Markdown(res)\n",
    "    else: print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLearner(get_model2(), dls, F.cross_entropy, lr=lr, cbs=[DeviceCB()]).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model = get_model2(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model3(act=nn.ReLU, nfs=(16,32,64,128,256), norm=nn.BatchNorm2d):\n",
    "    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [GlobalAvgPool(), nn.Linear(256, 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLearner(get_model3(), dls, F.cross_entropy, lr=lr, cbs=[DeviceCB()]).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[o.shape for o in get_model3()[0].parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model = get_model3(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model4(act=nn.ReLU, nfs=(16,32,64,128,256), norm=nn.BatchNorm2d):\n",
    "    layers = [conv(1, 16, ks=5, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [GlobalAvgPool(), nn.Linear(256, 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[o.shape for o in get_model4()[0].parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLearner(get_model4(), dls, F.cross_entropy, lr=lr, cbs=[DeviceCB()]).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model = get_model4(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 20 epochs without augmentation:\n",
    "\n",
    "```\n",
    "{'accuracy': '0.999', 'loss': '0.012', 'epoch': 19, 'train': True}\n",
    "{'accuracy': '0.924', 'loss': '0.284', 'epoch': 19, 'train': False}\n",
    "```\n",
    "\n",
    "With batchnorm, weight decay doesn't really regularize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_batch(b, tfm_x=fc.noop, tfm_y = fc.noop): return tfm_x(b[0]),tfm_y(b[1])\n",
    "\n",
    "tfms = nn.Sequential(transforms.RandomCrop(28, padding=4),\n",
    "                     transforms.RandomHorizontalFlip())\n",
    "\n",
    "augcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)\n",
    "model = get_model()\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=[SingleBatchCB(), augcb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = learn.batch\n",
    "print(xb.shape)\n",
    "show_images(xb[:16], imsize=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@fc.patch\n",
    "@fc.delegates(show_images)\n",
    "def show_image_batch(self:Learner, max_n=9, cbs=None, **kwargs):\n",
    "    self.fit(1, cbs=[SingleBatchCB()]+fc.L(cbs))\n",
    "    show_images(self.batch[0][:max_n], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_image_batch(max_n=16, imsize=(1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = nn.Sequential(transforms.RandomCrop(28, padding=1),\n",
    "                     transforms.RandomHorizontalFlip())\n",
    "augcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "epochs = 20\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), augcb]\n",
    "model = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom collation function could let you do per-item transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_path = Path('models')\n",
    "mdl_path.mkdir(exist_ok=True)\n",
    "torch.save(learn.model, mdl_path/'data_aug.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test time augmentation (TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CapturePreds(Callback):\n",
    "    def before_fit(self, learn): self.all_inps,self.all_preds,self.all_targs = [],[],[]\n",
    "    def after_batch(self, learn):\n",
    "        self.all_inps. append(to_cpu(learn.batch[0]))\n",
    "        self.all_preds.append(to_cpu(learn.preds))\n",
    "        self.all_targs.append(to_cpu(learn.batch[1]))\n",
    "    def after_fit(self, learn):\n",
    "        self.all_preds,self.all_targs,self.all_inps = map(torch.cat, [self.all_preds,self.all_targs,self.all_inps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@fc.patch\n",
    "def capture_preds(self: Learner, cbs=None, inps=False):\n",
    "    cp = CapturePreds()\n",
    "    self.fit(1, train=False, cbs=[cp]+fc.L(cbs))\n",
    "    res = cp.all_preds,cp.all_targs\n",
    "    if inps: res = res+(cp.all_inps,)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap1, at = learn.capture_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttacb = BatchTransformCB(partial(tfm_batch, tfm_x=TF.hflip), on_val=True)\n",
    "ap2, at = learn.capture_preds(cbs=[ttacb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap1.shape,ap2.shape,at.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = torch.stack([ap1,ap2]).mean(0).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round((ap==at).float().mean().item(), 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random erase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,_ = next(iter(dls.train))\n",
    "xbt = xb[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xm,xs = xbt.mean(),xbt.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt.min(), xbt.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "szx = int(pct*xbt.shape[-2])\n",
    "szy = int(pct*xbt.shape[-1])\n",
    "stx = int(random.random()*(1-pct)*xbt.shape[-2])\n",
    "sty = int(random.random()*(1-pct)*xbt.shape[-1])\n",
    "stx,sty,szx,szy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init.normal_(xbt[:,:,stx:stx+szx,sty:sty+szy], mean=xm, std=xs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(xbt, imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt.min(), xbt.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _rand_erase1(x, pct, xm, xs, mn, mx):\n",
    "    szx = int(pct*x.shape[-2])\n",
    "    szy = int(pct*x.shape[-1])\n",
    "    stx = int(random.random()*(1-pct)*x.shape[-2])\n",
    "    sty = int(random.random()*(1-pct)*x.shape[-1])\n",
    "    init.normal_(x[:,:,stx:stx+szx,sty:sty+szy], mean=xm, std=xs)\n",
    "    x.clamp_(mn, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,_ = next(iter(dls.train))\n",
    "xbt = xb[:16]\n",
    "_rand_erase1(xbt, 0.2, xbt.mean(), xbt.std(), xbt.min(), xbt.max())\n",
    "show_images(xbt, imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt.mean(),xbt.std(),xbt.min(), xbt.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def rand_erase(x, pct=0.2, max_num = 4):\n",
    "    xm,xs,mn,mx = x.mean(),x.std(),x.min(),x.max()\n",
    "    num = random.randint(0, max_num)\n",
    "    for i in range(num): _rand_erase1(x, pct, xm, xs, mn, mx)\n",
    "#     print(num)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,_ = next(iter(dls.train))\n",
    "xbt = xb[:16]\n",
    "rand_erase(xbt, 0.2, 4)\n",
    "show_images(xbt, imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RandErase(nn.Module):\n",
    "    def __init__(self, pct=0.2, max_num=4):\n",
    "        super().__init__()\n",
    "        self.pct,self.max_num = pct,max_num\n",
    "    def forward(self, x): return rand_erase(x, self.pct, self.max_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T04:22:57.799399516Z",
     "start_time": "2023-06-10T04:22:57.710089331Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m tfms \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\u001B[43mtransforms\u001B[49m\u001B[38;5;241m.\u001B[39mRandomCrop(\u001B[38;5;241m28\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m),\n\u001B[1;32m      2\u001B[0m                      transforms\u001B[38;5;241m.\u001B[39mRandomHorizontalFlip(),\n\u001B[1;32m      3\u001B[0m                      RandErase())\n\u001B[1;32m      4\u001B[0m augcb \u001B[38;5;241m=\u001B[39m BatchTransformCB(partial(tfm_batch, tfm_x\u001B[38;5;241m=\u001B[39mtfms), on_val\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "tfms = nn.Sequential(transforms.RandomCrop(28, padding=1),\n",
    "                     transforms.RandomHorizontalFlip(),\n",
    "                     RandErase())\n",
    "augcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=[DeviceCB(), SingleBatchCB(), augcb])\n",
    "learn.fit(1)\n",
    "xb,yb = learn.batch\n",
    "show_images(xb[:16], imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "lr = 2e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), augcb]\n",
    "model = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,_ = next(iter(dls.train))\n",
    "xbt = xb[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "szx = int(pct*xbt.shape[-2])\n",
    "szy = int(pct*xbt.shape[-1])\n",
    "stx1 = int(random.random()*(1-pct)*xbt.shape[-2])\n",
    "sty1 = int(random.random()*(1-pct)*xbt.shape[-1])\n",
    "stx2 = int(random.random()*(1-pct)*xbt.shape[-2])\n",
    "sty2 = int(random.random()*(1-pct)*xbt.shape[-1])\n",
    "stx1,sty1,stx2,sty2,szx,szy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt[:,:,stx1:stx1+szx,sty1:sty1+szy] = xbt[:,:,stx2:stx2+szx,sty2:sty2+szy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(xbt, imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _rand_copy1(x, pct):\n",
    "    szx = int(pct*x.shape[-2])\n",
    "    szy = int(pct*x.shape[-1])\n",
    "    stx1 = int(random.random()*(1-pct)*x.shape[-2])\n",
    "    sty1 = int(random.random()*(1-pct)*x.shape[-1])\n",
    "    stx2 = int(random.random()*(1-pct)*x.shape[-2])\n",
    "    sty2 = int(random.random()*(1-pct)*x.shape[-1])\n",
    "    x[:,:,stx1:stx1+szx,sty1:sty1+szy] = x[:,:,stx2:stx2+szx,sty2:sty2+szy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,_ = next(iter(dls.train))\n",
    "xbt = xb[:16]\n",
    "_rand_copy1(xbt, 0.2)\n",
    "show_images(xbt, imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def rand_copy(x, pct=0.2, max_num = 4):\n",
    "    num = random.randint(0, max_num)\n",
    "    for i in range(num): _rand_copy1(x, pct)\n",
    "#     print(num)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,_ = next(iter(dls.train))\n",
    "xbt = xb[:16]\n",
    "rand_copy(xbt, 0.2, 4)\n",
    "show_images(xbt, imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RandCopy(nn.Module):\n",
    "    def __init__(self, pct=0.2, max_num=4):\n",
    "        super().__init__()\n",
    "        self.pct,self.max_num = pct,max_num\n",
    "    def forward(self, x): return rand_copy(x, self.pct, self.max_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = nn.Sequential(transforms.RandomCrop(28, padding=1),\n",
    "                     transforms.RandomHorizontalFlip(),\n",
    "                     RandCopy())\n",
    "augcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=[DeviceCB(), SingleBatchCB(), augcb])\n",
    "learn.fit(1)\n",
    "xb,yb = learn.batch\n",
    "show_images(xb[:16], imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "epochs = 25\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), augcb]\n",
    "model = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn2 = TrainLearner(model2, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn2.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_path = Path('models')\n",
    "torch.save(learn.model,  mdl_path/'randcopy1.pkl')\n",
    "torch.save(learn2.model, mdl_path/'randcopy2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp1 = CapturePreds()\n",
    "learn.fit(1, train=False, cbs=cp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp2 = CapturePreds()\n",
    "learn2.fit(1, train=False, cbs=cp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = torch.stack([cp1.all_preds,cp2.all_preds]).mean(0).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.947"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round((ap==cp1.all_targs).float().mean().item(), 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.1\n",
    "dist = distributions.binomial.Binomial(probs=1-p)\n",
    "dist.sample((10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p=0.1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training: return x\n",
    "        dist = distributions.binomial.Binomial(tensor(1.0).to(x.device), probs=1-self.p)\n",
    "        return x * dist.sample(x.size()) * 1/(1-self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropmodel(act=nn.ReLU, nfs=(16,32,64,128,256,512), norm=nn.BatchNorm2d, drop=0.0):\n",
    "    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm), nn.Dropout2d(drop)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [nn.Flatten(), Dropout(drop), nn.Linear(nfs[-1], 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "epochs=5\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "model = get_dropmodel(act_gr, norm=nn.BatchNorm2d, drop=0.1).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.809</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.829</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.892</td>\n",
       "      <td>0.396</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.882</td>\n",
       "      <td>0.350</td>\n",
       "      <td>1</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.916</td>\n",
       "      <td>0.280</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.911</td>\n",
       "      <td>0.284</td>\n",
       "      <td>2</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.937</td>\n",
       "      <td>0.209</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.924</td>\n",
       "      <td>0.242</td>\n",
       "      <td>3</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.956</td>\n",
       "      <td>0.157</td>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.932</td>\n",
       "      <td>0.223</td>\n",
       "      <td>4</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTD_CB(Callback):\n",
    "    def before_epoch(self, learn):\n",
    "        learn.model.apply(lambda m: m.train() if isinstance(m, (nn.Dropout,nn.Dropout2d)) else None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[xl] = [(TF.to_tensor(o)*2-1) for o in b[xl]]\n",
    "\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.762</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.826</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.853</td>\n",
       "      <td>0.608</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.851</td>\n",
       "      <td>0.511</td>\n",
       "      <td>1</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.869</td>\n",
       "      <td>0.494</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.867</td>\n",
       "      <td>0.409</td>\n",
       "      <td>2</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.883</td>\n",
       "      <td>0.402</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.885</td>\n",
       "      <td>0.355</td>\n",
       "      <td>3</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.894</td>\n",
       "      <td>0.337</td>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.843</td>\n",
       "      <td>0.457</td>\n",
       "      <td>4</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.901</td>\n",
       "      <td>0.300</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.859</td>\n",
       "      <td>0.402</td>\n",
       "      <td>5</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.913</td>\n",
       "      <td>0.260</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.901</td>\n",
       "      <td>0.292</td>\n",
       "      <td>6</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.918</td>\n",
       "      <td>0.240</td>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.891</td>\n",
       "      <td>0.320</td>\n",
       "      <td>7</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.923</td>\n",
       "      <td>0.224</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.904</td>\n",
       "      <td>0.270</td>\n",
       "      <td>8</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.929</td>\n",
       "      <td>0.204</td>\n",
       "      <td>9</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.907</td>\n",
       "      <td>0.265</td>\n",
       "      <td>9</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.935</td>\n",
       "      <td>0.189</td>\n",
       "      <td>10</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.906</td>\n",
       "      <td>0.269</td>\n",
       "      <td>10</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.937</td>\n",
       "      <td>0.184</td>\n",
       "      <td>11</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.923</td>\n",
       "      <td>0.223</td>\n",
       "      <td>11</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.942</td>\n",
       "      <td>0.167</td>\n",
       "      <td>12</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.919</td>\n",
       "      <td>0.225</td>\n",
       "      <td>12</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.947</td>\n",
       "      <td>0.153</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.928</td>\n",
       "      <td>0.209</td>\n",
       "      <td>13</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.953</td>\n",
       "      <td>0.136</td>\n",
       "      <td>14</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.934</td>\n",
       "      <td>0.196</td>\n",
       "      <td>14</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.957</td>\n",
       "      <td>0.125</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.936</td>\n",
       "      <td>0.186</td>\n",
       "      <td>15</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.961</td>\n",
       "      <td>0.114</td>\n",
       "      <td>16</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.938</td>\n",
       "      <td>0.179</td>\n",
       "      <td>16</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.965</td>\n",
       "      <td>0.104</td>\n",
       "      <td>17</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.939</td>\n",
       "      <td>0.185</td>\n",
       "      <td>17</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.968</td>\n",
       "      <td>0.096</td>\n",
       "      <td>18</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.941</td>\n",
       "      <td>0.180</td>\n",
       "      <td>18</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.971</td>\n",
       "      <td>0.091</td>\n",
       "      <td>19</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.941</td>\n",
       "      <td>0.177</td>\n",
       "      <td>19</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAnOElEQVR4nO3deXxU1d3H8c8vewJhD1tYRRZBZJFVEVHRsilqfVr3aheq1VatrXWpW1utrdY+daUu6KNVbOveigIuqCiLARHCJjuENQRIAiRkO88fMxmyzGQGSDK54ft+vfLKzL1nZn4n4jc35557rjnnEBGRxiUm2gWIiEjtU7iLiDRCCncRkUZI4S4i0ggp3EVEGqG4aH1wmzZtXLdu3aL18SIinrRo0aLdzrm0cO2iFu7dunUjIyMjWh8vIuJJZrYpknYalhERaYQU7iIijZDCXUSkEYramLuIyNEoLi4mKyuLwsLCaJdSp5KSkujUqRPx8fFH9XqFu4h4SlZWFqmpqXTr1g0zi3Y5dcI5R05ODllZWXTv3v2o3kPDMiLiKYWFhbRu3brRBjuAmdG6detj+utE4S4intOYg73csfbRc+H+7c58Hp21mt37D0W7FBGRBstz4b5m534e+3gtOfuLol2KiByH9u3bx1NPPXXEr5swYQL79u2r/YJC8Fy4Hwd/jYlIAxYq3EtLS2t83YwZM2jRokUdVVWdZ2fLOHQHKRGpf7fffjvr1q1j4MCBxMfH07RpUzp06MCSJUtYsWIFF154IVu2bKGwsJCbbrqJKVOmAIeXXNm/fz/jx49n1KhRfPnll6Snp/POO++QnJxcq3V6LtzLD9x1d0ARuf8/y1mxLa9W37Nvx2bce36/kPsfeughMjMzWbJkCXPmzGHixIlkZmYGpixOmzaNVq1aUVBQwNChQ/nud79L69atK73HmjVrmD59Os8++yzf+973eOONN7jyyitrtR/eC3cNy4hIAzJs2LBKc9Efe+wx3nrrLQC2bNnCmjVrqoV79+7dGThwIACnnnoqGzdurPW6PBfu5XTkLiI1HWHXlyZNmgQez5kzhw8//JB58+aRkpLCmDFjgs5VT0xMDDyOjY2loKCg1uvy3AnV8oEZjbmLSDSkpqaSn58fdF9ubi4tW7YkJSWFVatWMX/+/Hqu7jDPHblrWEZEoql169acfvrpnHzyySQnJ9OuXbvAvnHjxjF16lROOeUUevfuzYgRI6JWp+fCvZyGZUQkWl599dWg2xMTE3n//feD7isfV2/Tpg2ZmZmB7b/61a9qvT7w4LCMDtxFRMLzXrj7x2V05C4iElrYcDezzmb2iZmtNLPlZnZTkDZjzCzXzJb4v+6pm3J15C4iviVxG7tj7WMkY+4lwK3OucVmlgosMrPZzrkVVdp97pybdEzVHAHNlhE5PiUlJZGTk9Ool/0tX889KSnpqN8jbLg757YD2/2P881sJZAOVA33elH+3/I4+MUtIkF06tSJrKwssrOzo11KnSq/E9PROqLZMmbWDRgELAiye6SZfQNsA37lnFse5PVTgCkAXbp0OeJife9xVC8TkUYiPj7+qO9OdDyJ+ISqmTUF3gBuds5VXcxhMdDVOTcAeBx4O9h7OOeecc4Ncc4NSUtLO8qS/e91TK8WEWncIgp3M4vHF+yvOOferLrfOZfnnNvvfzwDiDezNrVaaXkt5VeoalxGRCSkSGbLGPA8sNI592iINu397TCzYf73zanNQg9/mO+bol1EJLRIxtxPB64ClpnZEv+2O4EuAM65qcAlwPVmVgIUAJe6Ojq01pC7iEh4kcyWmUuYTHXOPQE8UVtFRUKjMiIioXn2ClUNzIiIhOa9cI92ASIiHuC5cC+nYRkRkdA8F+6m2TIiImF5L9w1MCMiEpbnwr2chmVERELzXLgfXjhM6S4iEor3wt3/XdEuIhKa58JdQ+4iIuF5L9z9NCojIhKa58I9sCqkBmZERELyXrhrWEZEJCzPhXuADtxFRELyXLhrtoyISHjeC3eNy4iIhOW5cC+n2TIiIqF5LtwPLxymdBcRCcV74e7/riN3EZHQvBfuGnIXEQnLc+FeTgfuIiKheTDc/VeoalxGRCQkz4W7hmVERMLzXLiX03G7iEhongv3wIG70l1EJCTvhbvGZUREwvJcuJfTRUwiIqF5Ltx1EZOISHjeC/fADbKjW4eISEMWNtzNrLOZfWJmK81suZndFKSNmdljZrbWzJaa2eC6KffwnZhERCS0uAjalAC3OucWm1kqsMjMZjvnVlRoMx7o6f8aDjzt/15ndOAuIhJa2CN359x259xi/+N8YCWQXqXZZOAl5zMfaGFmHWq9WioOyyjeRURCOaIxdzPrBgwCFlTZlQ5sqfA8i+q/AEREpJ5EHO5m1hR4A7jZOZdXdXeQl1Q7tDazKWaWYWYZ2dnZR1ZpuDcXEZGAiMLdzOLxBfsrzrk3gzTJAjpXeN4J2Fa1kXPuGefcEOfckLS0tKOpV7NlREQiEMlsGQOeB1Y65x4N0exd4Gr/rJkRQK5zbnst1nm4Ht0iW0QkrEhmy5wOXAUsM7Ml/m13Al0AnHNTgRnABGAtcBC4ttYr9dPqAyIi4YUNd+fcXIKPqVds44AbaquoSGhYRkQkNO9eoRrdMkREGjTvhbuuUBURCctz4V5OwzIiIqF5LtwPD8so3UVEQvFeuEe7ABERD/BcuJfTsIyISGieC3fNlhERCc9z4V4+MKNVIUVEQvNcuOsKVRGR8DwX7iIiEp7nwl03yBYRCc974a5xGRGRsDwX7uV0EZOISGieC3cNy4iIhOe9cNeojIhIWJ4L93I6chcRCc1z4V6+5K+yXUQkNO+Fe+AG2Yp3EZFQPBfuIiISnmfDXcftIiKheS7cA7NllO4iIiF5MNw1F1JEJBzPhXs5XaEqIhKa58JdV6iKiITnvXDXqIyISFieC/dyOnAXEQnNc+EeuEJV6S4iEpL3wj1wg2ylu4hIKN4L92gXICLiAWHD3cymmdkuM8sMsX+MmeWa2RL/1z21X2Z1GpYREQktLoI2LwJPAC/V0OZz59ykWqkonMCwjIiIhBL2yN059xmwpx5qiYhpYEZEJKzaGnMfaWbfmNn7ZtYvVCMzm2JmGWaWkZ2dfWyfqHEZEZGQaiPcFwNdnXMDgMeBt0M1dM4945wb4pwbkpaWdlQfZhqWEREJ65jD3TmX55zb7388A4g3szbHXFkIWn5ARCS8Yw53M2tv/qUazWyY/z1zjvV9a/g8AHbvP1RXHyEi4nlhZ8uY2XRgDNDGzLKAe4F4AOfcVOAS4HozKwEKgEtdHd4DLzk+FoBlW3Pr6iNERDwvbLg75y4Ls/8JfFMl60VyQiwntGlCXIxmzYiIhOK5K1QBmiTGUVqmQXcRkVA8Ge4xMUapsl1EJCRPhnusQZmO3EVEQvJmuMeYhmVERGrgyXCPMaNUE91FRELyZLjHxpiGZUREauDZcNeRu4hIaJ4M9xjTkbuISE08Ge46chcRqZknwz3GjNKyaFchItJweTLcY2M0z11EpCYeDXcNy4iI1MST4a4TqiIiNfNkuOvIXUSkZt4Md9PyAyIiNfFkuMfoClURkRp5MtxjtbaMiEiNPBnuMTGa5y4iUhNPhntsDJTpyF1EJCRvhrsZew4URbsMEZEGy5PhfqjENyazekd+lCsREWmYPBnuFwzoCMDWfQejXImISMPkyXDv3CoFgN37NTQjIhKMJ8O9ddMEAHbvPxTlSkREGiZPhntKQhwpCbHk6MhdRCQoT4Y7+I7ec3TkLiISlHfDvUmixtxFRELwbLi3aZqoMXcRkRA8G+5pqQk6chcRCSFsuJvZNDPbZWaZIfabmT1mZmvNbKmZDa79MqsrP3LX0buISHWRHLm/CIyrYf94oKf/awrw9LGXFV5yQiwAQ/7wYX18nIiIp4QNd+fcZ8CeGppMBl5yPvOBFmbWobYKDKWgqDTwuKhES0SKiFRUG2Pu6cCWCs+z/NuqMbMpZpZhZhnZ2dnH9KGTB3YMPC4sKa2hpYjI8ac2wt2CbAu6Hq9z7hnn3BDn3JC0tLRj+tAT26YGHu/OP0SxFngXEQmojXDPAjpXeN4J2FYL7xuxs//yKTe+urg+P1JEpEGrjXB/F7jaP2tmBJDrnNteC+8b1u8m9ws8nrl8Z318pIiIJ8SFa2Bm04ExQBszywLuBeIBnHNTgRnABGAtcBC4tq6KrUo3YxIRCS5suDvnLguz3wE31FpFR6B8dUgREanMs1eoAkzs34FmSWF/P4mIHHc8He5mxvPXDI12GSIiDY6nwx1gaLdWjOvXHoBd+YVRrkZEpGHwfLgDXOC/oOmvs9dEuRIRkYahUYR7qn/cffrCzVGuRESkYWgU4d61VZPA4xte0cVMIiKNIty7tE7hb5cOBOC9Zdu1FIGIHPcaRbgD9Gp3eK2Zf2VsqaGliEjj12jC/aQOzZh2zRAA7nork6821rRKsYhI49Zowh3g7D7tAo/fXJwVxUpERKKrUYU7wMI7zwFg+sItLM3aF91iRESipNFdu9+2WVLg8bx1OWzZU8BNr31NSZnj1nN78fNzekaxOhGR+tHojtwremNxFje8upiSMt/ykX+Z/W2UKxIRqR+NMtybJ8cD8O3O/ZW2J8fHRqMcEZF61yjDfck95wbdXlBcSnb+oXquRkSk/jXKcDezCo8r7xv6wIf1XI2ISP1rlOEOEOMP9cz7vkN8bOWEz9yaG4WKRETqT6ObLVPu3RtH8fma3TRJjKNtahJb9xUE9k16fC4T+rcnZ38Rj102iHYVZtiIiDQGjfbI/eT05lw/pgcA/To2q7Z/xrIdLNiwh+EPflTfpYmI1LlGe+Re0aPfH8jiTXsZ2aM1X23Yw+XPLai0v7i0jPjYGP6VsYUV2/K474J+UapURKR2NNoj94qaJsYxulca8bExDO3eiosGpVfa3/Ou91m8eS+3vb6UF7/cGJ0iRURq0XER7hXFx8bw1+8PrLZ97prdgcfz1uWw/1BJPVYlIlK7jrtwD+XRClevXvbsfG57/RsKikpZtSOPu95axiMzV0exOhGRI2POuah88JAhQ1xGRkZUPhtg+bZcnp6zjgcv7s+dby7jv0u3V2sTF2OBpQsANj40sT5LFBGpxswWOeeGhGt33B659+vYnCcuH0yzpHiuGN41aJuKwR7M8m25/OmDVUTrF6SISCjHbbhXNLRby4javTRvYyDIn/1sPRMfm8vTc9ZRUFwKwJdrd3OopLTO6hQRiZTCHYiLjWHhXefw4rVDa2x3zzvL6X7HDNbuyueBGSsD2w8WlbJ8Wy6XP7eAP85YVdflioiEdVzMc49E29QkspILwjcE7v/PikrPX5m/mb9+6Dshu373gVqvTUTkSEV05G5m48xstZmtNbPbg+wfY2a5ZrbE/3VP7Zda9xLjDv84LhzYMfD4jvF9+NV5vQLPu7RKqfS6Jz9ZG3j82bfZdVihiEhkwh65m1ks8CRwLpAFfGVm7zrnVlRp+rlzblId1Fhv+nZoxh3j+3DR4HTSmiZy8eBOLNuay9Uju5GcEMsjs3xH5+XrxZcrKi2LRrkiIiFFcuQ+DFjrnFvvnCsCXgMm121Z0WFm/PTMHrRNTcLMGN0rjRvOOpHkBN9NPjo09y0w9tScdTW+z868QopLy9h7oChkm+LSMvbUsF9E5FhEEu7pwJYKz7P826oaaWbfmNn7ZtYoF2eZd8c5lZ4/eFH/oO2GP/gR3/v7PAb9fjZlZY5PVu/i3xlbKrW57fWlDPbvFxGpbZGEuwXZVjWRFgNdnXMDgMeBt4O+kdkUM8sws4zsbO+PTcfFGOcP8I3Nf3PPeZX2fb15HwA5B4q49oWv+PXrSyvtf2fJVgByC4rrvlAROe5EEu5ZQOcKzzsB2yo2cM7lOef2+x/PAOLNrE3VN3LOPeOcG+KcG5KWlnYMZUdfi5R4zujVhocvOYUZvziD5inxgRuEVPT15r1BX5/kv59rzoEi+t83kz/6p1Yu2rSH5z5fX2d1i8jxIZJw/wroaWbdzSwBuBR4t2IDM2tv/nvbmdkw//vm1HaxDUFsjHFq15Ysuec8OjRPJik+lr7+9eI/uHk0bVMTK7Wf8vKiwOPyC6C+3ZnPwSLfxU7b9hWQX1jC3z/zBfp3n57HH95biYjIsQg7W8Y5V2JmNwIzgVhgmnNuuZld598/FbgEuN7MSoAC4FLXSK/JX/OH8SH39WqXyvw7zuHG6YuZsWxHtf3d75jBref2Ymd+YWDb1dMWBh5XHKIpK3PExBjOOUrLHHGxut5MRCJ33C4cVtdKSss48a73j+g1sTFGqf8Ea+b936FpYhy/+vc3vL9sOx/dOoZ2zRIr3fxbRI4/WjgsyuJiY3j9upFH9JrSCjNnDvjXk399URYHikoZ8cePeH7uhlqtUUQaL4V7HRrSrRUbH5rIrFtGB91//ZgedGwe/ObcwW4WMnvFzpCftSuvkDU784+uUBFpdBTu9aBXu9Rq20ae0JrbvtM75Gs+/zabUX/6uNK2BRv2kFtQTHGQK2JHP/wJ5/71s2MvVkQaBYV7PZl65eBKz/MKiwNXwQZz339WkLW3+kJmA+6fRc+73ucnL2WwOedgYHthsZZAEJHDFO71ZNzJHfj8trN46gpfyJ/UwTd98r4L+jE7xLBNTWav2Mk1Ly6stv2HL37FxjArU+7ILWT5tlw+yKw+o0dEGgfNlomCpVn76Nk2NbBmDfju6rQjt5CX5m3i0yNYWfK6M3sw9dPga920TIln+pQR9GnfLLCtuLSMnhVm8ax9YDxxsTE453hlwWYmndKBFikJR9ErEakPkc6WUbg3UNPmbmDPgSKeqLCc8NHo2jqFe8/vS9fWTUhvkUyfuz+otL98yuXSrH1c8MQXjOvXntvG9SY+NobC4lLeWbKNW8/rpSmYIg1EpOGum3U0UD8c1R2gWrh3bZ3C3RP78uOXIvvFuCnnID980df28uFdqu1/beFmHp65mkMlvjH7zG25nP2XT6vV0qqJjuZFvERj7g1c5v3foVe7poHnD118CmP7tqvWruoJ22BeXbC52rY/vLcyEOxA0JO45UsTv/31Vl6at5ELnphbbarm9f9YpDVxRBoQhXsD1zQxjlm3nMkF/tUnK47TA1w6tDNzfjWGM3pWn3UTSeBHYuyjn7Jo0x5u/ucS7nlnOUuzcnn7662V2ryfuYM/vLeSbfsKmLtmd7X3mL1iJ8uyclmwvuYlh37z+lL+Mmt1rdQtcjzTsIxHPHDRyZzRsw0DOjUH4PHLBjHtiw388eL+gfHwJfecy8DfzQZg1e/HkRBiPZrfX3gyd7+deUSf/92n51V6/tu3M2mREs8TH6/lkf8ZENg+6fG57DlQxGOXDfLVsT2PM3qm8ZMKw0gzbx7NjrxCnvt8Pf937TBiYoyikjKKSsv4p3/d+1vPC34NwKOzVtM8JYEf+YetRCQ4nVBtZLrd/h4AGx+aCPjWjf9mSy7TvvAtXXBGzzac1699pXB/82encfFTX9ZbjSekNWF9tm+65sI7z2FpVm61cwjl9QPsPVDEmEfm8MxVp/L9Z+ZX2n/WI3O45NRO3HDWifVUfXhb9xWQ3iI52mVII6W1ZY5Td4zvw4ltD4/RTx6Yzhk9Dy+t/+zVQ7jglI70ae+7anZcv/YM7tKScf3a88PTu9PPv3zxNad1C/r+Pz/72EO0PNgBLnrqy6Anh9dn7+etr7MoKinjy3U55BYUB4IdYO2u/XS7/T027D7AwzOrD+P866stvLJg0zHXGs6FT35R6VzDf5du4/SHPubLddWHpkTqk4ZlGpmfntmDn57Zo9K29Ja+o8jbxvUmKT6WpPhYPrh5NEUlZSTE+X6/T73qVABy9h/iqTnr+PV3ejPxlA5c/ux8iksP/3V3y9herM8+wHvLttdKvVv3VT+BC/DgjJV8uHIXz32+odoa+QD/mF89uItKyrjln0v48Rndue0N352vrhjelUufmce3O/cz65bRtGmaSElpGQeLS2maEMfzczewK7+Q+NgYnp+7gYzfjiU1Kb7ae4eyZMs+lmzZx4/POAGAL9b6Qn1d9gGS42OZ+uk6nrx8sJZslnqnYZnjxLrs/XRv3YSYYLeLqsGuvEKGPfgR4BvSeflHwwEY8/AnbMw5yAc3n8Gjs75lVg2LmtWFhLgYiirM8vnu4E5s21fAvConbB+8qD93vrUs8PzxywbxxdrdvPZV5XvaVtQ2NZGzerflT5ecUm1fWZlj7trdDD+hFTn7izjtId/6P6t+P46bXvuajI17yTlQxDWndeODzB3syCtk7m/OolPLlIj6VVBUSlJ8jK4rkJB0EZPUisLi0sCFTxXHwQuLS9meW0j3Nk1wzvHnmatZsD6Hxf57xwbTpmkCFw1K59nPvbF08QvXDqVfh2akpfrW0T9YVMKrCzYHvVNW6yYJ5PinjFZ1/ZgeXD6sC51b1Rzwh0pK6f3bD/jxqO70S2/G3gPF/HBUd75ct5v2zZI4Ia1pja+v6rWFmzmlUwv6dmzGlj0H6dgimdgj/OUuDY/CXWpN1ZO0Ndl3sIiYGOOU+2ZV2/fzs0/k1vN6U1xaxug/f0LPdql8s2VfRDcJv+GsHjz5SeVlFn478aR6uyXhsO6tWLdrf8gAj8SE/u156opTcc5x51vLmLt2N+P6tefZzzdwzWnduH5MD4b7/0oKJpKff7mDRSX0vWcmzZLimHnLaEb+8WN+NqYHt43rc9T1S8OgcJdac9XzCzivbzuuGtkt4tfsyC1kxB8PB9Ub14+kf3qLwBh/udIyxwPvrWTaFxvo0DyJd248nX9nZFU6Sfr0FYMZ378DO/MKue4fi7h5bC/SWyTRvU1TJj85l8ytecfcx4ageXJ8jb/oxp7Ulg9X7uK/Px/Fy/M20TQpjkuHdqZHWtNqw22ffpvND6YtpGVKPC//aDiTHp/LSR2aMf0nwzEzmieHP6+wPns/L8/fxN0T+x7xcJ7UHYW7RF23298jNsZ4/bqRDOrSMmS7sjJH1t4C2jdPIiEuhoKiUq55YSELNuwBfNMl2zYLflMTgA27D7B4015u/fc3/O3SgfzuPyuqHWE/dHF/khNiuem1JQCkt0gOeTLXa346+gRuH9+HH7zwFYlxMZVu6tI2NZE/X3IK17zwVaXX3DmhD2NPakeb1ESaJsRVCu9Fm/Ywe8UuXp63kQNFpUz/yQi+XLebm8f2imhYZ8ueg6SlJpIU77vgrqikjPhYC3kewTnHU3PWcfHgdDo0994U0g9X7GRkj9Y0Sayf+SkKd4m6zK25dG6ZQvOUyGefVPTsZ+t5YMZKVv1+XCAoIpG19yA/n/41X1cY/y8f0rjsmfnMW5/Dhj9OoPsdMwCYdEoHLh6cTs+2qezMK+SSqZUv2HrmqlOZ8vIiAG46pydjT2rH+U/MjaiWiwal81aVq3kj1apJQmDph7qSHB9LQXEpt43rTdvUJC4alE6PO2dUatO5VTJb9hRw7/l9adUkgZXb8xl7UlsGd2kZ+KXgnMPMAvcOPrdvO569egj3vJPJS/M2ceu5vbhiRNdqaxQ557jv3eX837xNDOnaktevPy1krW8symL6ws01tqlv67L3c85fPuX8AR153H/hXl1TuMtxbcW2PCY89jk/GtWdbm2acNWIroBvNkrOgUN0apnC64uy+OdXm5n+kxGVpipWPIkM8M2951Fa5khJiA38kik/DzGmdxpzVvuWaP795H7c/c5yAEb3SuN7QzoxsX8H/jr7Wx772LcAXN8OzXj3xtM5UFTKgPurn5fwmvMHdGRQ5xb87aM13DXhJM7sncbwBz8iLsZY88D4wC/QijJ+O5Y2TRNxzrE0K5fJT34BQI+0Jnz4yzNxDmJiDOcc//xqC5MGdKRpYlzgZ77+wQkRDRO9v2w7W/cV8P2hnXlt4RZ+NKp7xMNL+YXFLNq0lzG929bYLnNrLpMen0uf9ql8cHPw+zLkHiwmPs5ISaidI3uFu8gxuOPNZSzckMPkgen84pye1fbPXrGTn7yUwYvXDuXLdTmUlTl+O6kv3W5/j5SEWFb8blygrXOOnXmHeGXBJv7n1M50ae2bNfOvjC18sXY3HZonc/mwLox++JNKn1F+VA2+4MvaW1Bpkbc+7VNZtaNh3Tf3jJ5t+DzI2kJV23Rv04SX5m3iiuFdeMW/oN0JbZqQnX+I/EMl/OfGUew5WMQPpvluSFPxqmaAD385mhPbVr595fbcAtKaJrI9t5DSMseYR+YAcOWILvxj/mbuv6Af7ZolMqx7azI27qFpYhw78gq5eHAndu8/RHxsDM2T43HOcfK9MzlQVMqCO8+hXQ1DguXhfmLbpnz4yzODtul2+3t0apnM3N+cHfbnFwmFu0gdKyktq3Zx0q78QuJiYo5qieQPMrdz3T8WA3DjWSdyXr92PPHxWs7u05ZLh/mWa96VV8jN/1zC/146kBbJCfz1w2+Jj43hsY/WVHqvlinx/PmSAezKL+Sut3xLTVRce6gx+Pruc2nZJIHMrbk8Mmt14C+oqlISYjlYVBp4npaaSHb+ocDz//58FJMen0taaiJv/ew0Rv3p8C/Z7w/pzPa8Qp656lTyC0uIjTFaNUmgsLiUldvzKCopC1w5/fGtZ1abrrp1XwGn+6+FOJLZTjVRuIt40KJNe5mzelfIhdOCWZaVW+kcwDs3nM6Azi0Cz9fuyidrbwFjerdl1vIdTP10XeB6hP/+fBQlZY4L/UMj5X46+gQ+WrWL9s2SmLv26JZSiIsxSsrqLl+GdW/F6h35EU2lrU0zfnEGUz9dx7vfbKu279y+7bhyRFe27SugR1pT7n47k9U78yu9tmWT+GM6caxwFzlOlJU5Hpm1mitGdCU1KY5mESyfsHbXft5ZspVbxvYiJsa47uVFJCfEBk7+lh9lFhaXsnnPQdZnH+C6fyzi1R8PJzkhlk9W7QqcR6jqF2efyI9GnUB8nNH3npkAfPbrsyoNOx1J8F8+vEvQexF42Tf3nHfUEw0U7iJyxFbvyKdLq5Rq9w0A3/134/3DUM45DpWUkVdYzLAHKl94NfXKUxl3cnvAN9488oTWvHDtUE79/WzuntSXAZ1bcFKHZizevLfaaqTl015zC4q5/93l7DlYxAvXDGX2ip2BGUuR6twqmYOHSjnnpLb8KyPriF5b1164Zihn9an5ZG0oCncRqRd7DxTx6bfZpLdMprC4lFEntgnMac8tKCYpPobEuOBTWYf8YTZjT2rHWX3a8smqXTz03err+ZTL2X+InXmHuO/d5bRsEs/M5dXXMzp/QEfumdSX4tIyWqYkBH5Jbco5wKsLN/P3T9czsX+HiBe++8OFJ/Pm4qwal9UA6NA8ie25hYHn5RechfLgRf2D3vYyEgp3EWm0duUXMuyBjxjQqTnXjzmRrq1TaJIQF5iJVJMv1+7m8ucWAPDBzWdwqLiMpklxNEmIY9WOPOav38PYk9oypFsrABZu2MP3/j6Pif07cPekvrRvnsQPX/yKj1f5wvutn51Gi5QEfv3vb/jZWT04/cQ2JMbFsjRrH7vyDjG2bzvOemQOG3Yf4Nff6c3DM1fzi3N68stzex1V3xXuIiIhzFy+gxXb8rh5bM+jXoHzy7W76dQyJaJfKAeLSnjhi438dPQJ/OmDVQzv3jrovZAjUavhbmbjgL8BscBzzrmHquw3//4JwEHgGufc4preU+EuInLkau1OTGYWCzwJjAf6ApeZWd8qzcYDPf1fU4Cnj7hiERGpNZHcHmYYsNY5t945VwS8Bkyu0mYy8JLzmQ+0MLMOtVyriIhEKJJwTwcq3rYmy7/tSNtgZlPMLMPMMrKzg19NJiIixy6ScA92tqHqQH0kbXDOPeOcG+KcG5KWlhZJfSIichQiCfcsoHOF552AqtfdRtJGRETqSSTh/hXQ08y6m1kCcCnwbpU27wJXm88IINc5F9lVAiIiUuvCLjDsnCsxsxuBmfimQk5zzi03s+v8+6cCM/BNg1yLbyrktXVXsoiIhBPR6vHOuRn4ArzitqkVHjvghtotTUREjlbUrlA1s2xg01G+vA1wdOuQNkzqT8PVmPoC6k9DFmlfujrnws5IiVq4Hwszy4jkCi2vUH8arsbUF1B/GrLa7kskJ1RFRMRjFO4iIo2QV8P9mWgXUMvUn4arMfUF1J+GrFb74skxdxERqZlXj9xFRKQGCncRkUbIc+FuZuPMbLWZrTWz26NdTzhm1tnMPjGzlWa23Mxu8m9vZWazzWyN/3vLCq+5w9+/1Wb2nehVH5qZxZrZ12b2X/9zT/bHzFqY2etmtsr/32ikV/sCYGa3+P+dZZrZdDNL8lJ/zGyame0ys8wK2464fjM71cyW+fc9Zkd7u6VjFKI/D/v/vS01s7fMrEWFfbXXH+ecZ77wLX+wDjgBSAC+AfpGu64wNXcABvsfpwLf4rvpyZ+B2/3bbwf+5H/c19+vRKC7v7+x0e5HkH79EngV+K//uSf7A/wf8GP/4wSghYf7kg5sAJL9z/8FXOOl/gCjgcFAZoVtR1w/sBAYiW/F2veB8Q2oP+cBcf7Hf6qr/njtyD2SG4c0KM657c5/y0HnXD6wEt//hJPxBQv+7xf6H08GXnPOHXLObcC3Xs+wei06DDPrBEwEnquw2XP9MbNm+P7nex7AOVfknNuHB/tSQRyQbGZxQAq+1Vk90x/n3GfAniqbj6h+/42Cmjnn5jlfMr5U4TX1Klh/nHOznHMl/qfz8a2iC7XcH6+Fe0Q3BWmozKwbMAhYALRz/pUz/d/b+pt5oY//C9wGlFXY5sX+nABkAy/4h5ieM7MmeLMvOOe2Ao8Am4Ht+FZnnYVH+1PBkdaf7n9cdXtD9EN8R+JQy/3xWrhHdFOQhsjMmgJvADc75/JqahpkW4Ppo5lNAnY55xZF+pIg2xpKf+Lw/cn8tHNuEHAA35/9oTTkvuAfi56M70/6jkATM7uyppcE2dZg+hOBUPV7ol9mdhdQArxSvilIs6Puj9fC3ZM3BTGzeHzB/opz7k3/5p3+P7fwf9/l397Q+3g6cIGZbcQ3LHa2mf0Db/YnC8hyzi3wP38dX9h7sS8AY4ENzrls51wx8CZwGt7tT7kjrT+Lw0MdFbc3GGb2A2AScIV/qAVquT9eC/dIbhzSoPjPaj8PrHTOPVph17vAD/yPfwC8U2H7pWaWaGbdgZ74TqY0CM65O5xznZxz3fD9/D92zl2JB/vjnNsBbDGz3v5N5wAr8GBf/DYDI8wsxf/v7hx853i82p9yR1S/f+gm38xG+H8OV1d4TdSZ2TjgN8AFzrmDFXbVbn+icQb5GM8+T8A342QdcFe064mg3lH4/oRaCizxf00AWgMfAWv831tVeM1d/v6tJkpn+SPs2xgOz5bxZH+AgUCG/7/P20BLr/bFX9/9wCogE3gZ38wLz/QHmI7vfEExviPWHx1N/cAQ/89gHfAE/qvxG0h/1uIbWy/Pg6l10R8tPyAi0gh5bVhGREQioHAXEWmEFO4iIo2Qwl1EpBFSuIuINEIKdxGRRkjhLiLSCP0/J2y5oqJBonoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "epochs = 20\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), augcb]\n",
    "model = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model, 'models/data_aug2.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
